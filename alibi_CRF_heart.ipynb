{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a0ae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\dice_XAI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from alibi.explainers import CounterfactualRLTabular, CounterfactualRL\n",
    "from alibi.datasets import fetch_adult\n",
    "from alibi.models.tensorflow import HeAE\n",
    "from alibi.models.tensorflow import Actor, Critic\n",
    "from alibi.models.tensorflow import ADULTEncoder, ADULTDecoder\n",
    "from alibi.explainers.cfrl_base import Callback\n",
    "from alibi.explainers.backends.cfrl_tabular import get_he_preprocessor, get_statistics, \\\n",
    "    get_conditional_vector, apply_category_mapping\n",
    "import random\n",
    "\n",
    "def set_seed(s=0):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    tf.random.set_seed(s)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d79456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional, Tuple, Union\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from alibi.utils.data import Bunch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "df = pd.read_csv('E:\\Graduation_Project\\datasets\\heart.csv')\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "df[categorical_features] = df[categorical_features].astype(str)\n",
    "\n",
    "\n",
    "def fetch_heart(\n",
    "    features_drop: Optional[list] = None, \n",
    "    return_X_y: bool = False, \n",
    "    categorical_features: Optional[list] = None\n",
    ") -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]:\n",
    "    \n",
    "    raw_data = df\n",
    "    # get labels, features and drop unnecessary features\n",
    "    labels = (raw_data['target'] == 1).astype(int).values\n",
    "\n",
    "    if features_drop is None:\n",
    "        features_drop = []\n",
    "    features_drop = features_drop + ['target']\n",
    "\n",
    "    data = raw_data.drop(features_drop, axis=1)\n",
    "    features = list(data.columns)\n",
    "\n",
    "    # 关键部分：外部指定优先，否则自动推断\n",
    "    if categorical_features is None:\n",
    "        categorical_features = [f for f in features if data[f].dtype == 'O']\n",
    "\n",
    "    category_map = {}\n",
    "    for f in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        data_tmp = le.fit_transform(data[f].values)\n",
    "        data[f] = data_tmp\n",
    "        category_map[features.index(f)] = list(le.classes_)\n",
    "\n",
    "    # only return data values\n",
    "    data = data.values\n",
    "    target_names = [0, 1]\n",
    "\n",
    "    if return_X_y:\n",
    "        return data, labels\n",
    "\n",
    "    return Bunch(\n",
    "        data=data, \n",
    "        target=labels, \n",
    "        feature_names=features, \n",
    "        target_names=target_names, \n",
    "        category_map=category_map\n",
    "    )\n",
    "\n",
    "heart = fetch_heart(\n",
    "    categorical_features=categorical_features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ebd088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1️⃣ 划分类别型和数值型特征\n",
    "\n",
    "\n",
    "# Separate columns in numerical and categorical.\n",
    "categorical_names = [heart.feature_names[i] for i in heart.category_map.keys()]\n",
    "categorical_ids = list(heart.category_map.keys())\n",
    "\n",
    "numerical_names = [name for i, name in enumerate(heart.feature_names) if i not in heart.category_map.keys()]\n",
    "numerical_ids = [i for i in range(len(heart.feature_names)) if i not in heart.category_map.keys()]\n",
    "\n",
    "# Split data into train and test\n",
    "X, Y = heart.data, heart.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f061d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "# Define numerical standard scaler.\n",
    "num_transf = MinMaxScaler()\n",
    "\n",
    "# Define categorical one-hot encoder.\n",
    "cat_transf = OneHotEncoder(\n",
    "    categories=[range(len(x)) for x in heart.category_map.values()],\n",
    "    handle_unknown=\"ignore\"\n",
    ")\n",
    "\n",
    "# Define column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transf, numerical_ids),\n",
    "        (\"cat\", cat_transf, categorical_ids),\n",
    "    ],\n",
    "    sparse_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2d9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit preprocessor.\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Preprocess train and test dataset.\n",
    "X_train_ohe = preprocessor.transform(X_train)\n",
    "X_test_ohe = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8056ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "def build_simple_dnn():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(16, activation='relu', input_shape=(31,)))  # 输入31维特征\n",
    "    model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "model = build_simple_dnn()\n",
    "model.load_weights('my_model_weights.h5')   # 加载之前保存的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f40e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84857166, 0.15142827],\n",
       "       [0.88690937, 0.11309064]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define prediction function.\n",
    "predictor = lambda x: model.predict(preprocessor.transform(x), verbose=0)\n",
    "\n",
    "predictor(X_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c04846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Compute accuracy.\n",
    "acc = accuracy_score(y_true=Y_test, y_pred=predictor(X_test).argmax(axis=1))\n",
    "print(\"Accuracy: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9da2457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'trestbps', 'chol', 'thalach', 'oldpeak']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4312416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attribute types, required for datatype conversion.\n",
    "feature_types = {'age': int, 'trestbps': int, 'chol': int, 'thalach': int, 'oldpeak': float}\n",
    "\n",
    "# Define data preprocessor and inverse preprocessor. The invers preprocessor include datatype conversions.\n",
    "heae_preprocessor, heae_inv_preprocessor = get_he_preprocessor(X=X_train,\n",
    "                                                               feature_names=heart.feature_names,\n",
    "                                                               category_map=heart.category_map,\n",
    "                                                               feature_types=feature_types)\n",
    "\n",
    "# Define trainset\n",
    "trainset_input = heae_preprocessor(X_train).astype(np.float32)\n",
    "trainset_outputs = {\n",
    "    \"output_1\": trainset_input[:, :len(numerical_ids)]\n",
    "}\n",
    "\n",
    "for i, cat_id in enumerate(categorical_ids):\n",
    "    trainset_outputs.update({\n",
    "        f\"output_{i+2}\": X_train[:, cat_id]\n",
    "    })\n",
    "    \n",
    "trainset = tf.data.Dataset.from_tensor_slices((trainset_input, trainset_outputs))\n",
    "trainset = trainset.shuffle(1024).batch(32, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8865daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define autoencoder path and create dir if it doesn't exist.\n",
    "heae_path = os.path.join(\"tensorflow_heart\", \"ADULT_autoencoder\")\n",
    "if not os.path.exists(heae_path):\n",
    "    os.makedirs(heae_path)\n",
    "\n",
    "# Define constants.\n",
    "EPOCHS = 50             # epochs to train the autoencoder\n",
    "HIDDEN_DIM = 128         # hidden dimension of the autoencoder\n",
    "LATENT_DIM = 20          # define latent dimension\n",
    "\n",
    "# Define output dimensions.\n",
    "OUTPUT_DIMS = [len(numerical_ids)]\n",
    "OUTPUT_DIMS += [len(heart.category_map[cat_id]) for cat_id in categorical_ids]\n",
    "\n",
    "# Define the heterogeneous auto-encoder.\n",
    "heae = HeAE(encoder=ADULTEncoder(hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM),\n",
    "            decoder=ADULTDecoder(hidden_dim=HIDDEN_DIM, output_dims=OUTPUT_DIMS))\n",
    "\n",
    "# Define loss functions.\n",
    "he_loss = [keras.losses.MeanSquaredError()]\n",
    "he_loss_weights = [1.]\n",
    "\n",
    "# Add categorical losses.\n",
    "for i in range(len(categorical_names)):\n",
    "    he_loss.append(keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "    he_loss_weights.append(1./len(categorical_names))\n",
    "\n",
    "# Define metrics.\n",
    "metrics = {}\n",
    "for i, cat_name in enumerate(categorical_names):\n",
    "    metrics.update({f\"output_{i+2}\": keras.metrics.SparseCategoricalAccuracy()})\n",
    "    \n",
    "# Compile model.\n",
    "heae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "             loss=he_loss,\n",
    "             loss_weights=he_loss_weights,\n",
    "             metrics=metrics)\n",
    "\n",
    "if len(os.listdir(heae_path)) == 0:\n",
    "    # Fit and save autoencoder.\n",
    "    heae.fit(trainset, epochs=EPOCHS)\n",
    "    heae.save(heae_path, save_format=\"tf\")\n",
    "else:\n",
    "    # Load the model.\n",
    "    heae = keras.models.load_model(heae_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8481fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数0: adult_encoder_1/dense_22/kernel:0, 形状: (31, 128)\n",
      "[[ 0.08873643  0.0709216  -0.17608166 ... -0.03643989  0.14825208\n",
      "  -0.03980404]\n",
      " [ 0.09606059  0.0939892  -0.01512873 ... -0.11515966 -0.14141242\n",
      "  -0.15575741]\n",
      " [ 0.11487781  0.0382008  -0.06492206 ... -0.12050369 -0.09879702\n",
      "  -0.00706437]\n",
      " ...\n",
      " [ 0.14272259 -0.17787579 -0.06545106 ...  0.13873093 -0.20366322\n",
      "  -0.11280332]\n",
      " [ 0.1729906   0.02108923  0.06432234 ...  0.22458002  0.00874016\n",
      "  -0.09674485]\n",
      " [-0.08170243 -0.14469355  0.15328376 ...  0.03196003 -0.10160149\n",
      "  -0.16604851]]\n",
      "参数1: adult_encoder_1/dense_22/bias:0, 形状: (128,)\n",
      "[ 0.05174459 -0.01869852  0.00749912  0.08547264  0.02847661  0.0195519\n",
      "  0.05018203  0.05286779  0.05473476  0.07046813  0.03825403  0.03388041\n",
      "  0.0450633   0.0452936   0.06294727  0.05906113 -0.02772297  0.01984996\n",
      " -0.01549751  0.02406473  0.04475469  0.03362958  0.0284313   0.02166346\n",
      "  0.07286802  0.08531335 -0.00974583  0.04065216  0.00046673  0.01511034\n",
      "  0.05609525  0.05653055  0.02226181  0.0252516   0.04737053 -0.00167882\n",
      "  0.02601179  0.0204211   0.05579004  0.06274309 -0.03083405 -0.02157681\n",
      "  0.0269061   0.02831804 -0.03433107  0.00128752  0.00575476  0.02159594\n",
      "  0.07116317  0.03804918  0.02741654  0.05265228  0.05387383  0.02958434\n",
      "  0.05196484  0.008156    0.04852452  0.05613072  0.05277223  0.0460937\n",
      "  0.0503145   0.0631822   0.08118662  0.03677933  0.01871783  0.02993736\n",
      "  0.08244476  0.04122177 -0.0129362   0.0273151   0.04719114  0.04933484\n",
      "  0.06855252  0.01644229  0.06165004  0.05247221 -0.02686816  0.08805697\n",
      "  0.09657059  0.01773784  0.0505256   0.03221371  0.06477586  0.08190282\n",
      "  0.04955171  0.02859088  0.05106222 -0.00212057  0.00940579 -0.00815707\n",
      "  0.10593637  0.04149774  0.04885974  0.04910801  0.04111986  0.06818073\n",
      "  0.04305026 -0.01732822  0.02784047  0.00719957  0.07034393  0.04971496\n",
      "  0.01964288  0.06127994  0.05366837  0.03838321  0.03652518 -0.01798602\n",
      "  0.05494169 -0.00905094  0.01977646  0.05367241  0.03763645  0.05481358\n",
      "  0.03818248  0.06980515  0.09586386  0.03913891  0.01212231  0.0208143\n",
      "  0.05987517  0.07800911 -0.01331218  0.02313495  0.04930645  0.04776219\n",
      "  0.03545817 -0.02146257]\n",
      "参数2: adult_encoder_1/dense_23/kernel:0, 形状: (128, 20)\n",
      "[[ 0.1055098  -0.1448265  -0.15452038 ...  0.1037716   0.04294408\n",
      "  -0.11971129]\n",
      " [-0.04269735  0.165665    0.14659648 ... -0.06458919  0.01038874\n",
      "  -0.02577313]\n",
      " [ 0.05277601 -0.01452064  0.23575321 ... -0.1379205   0.15831852\n",
      "  -0.20316625]\n",
      " ...\n",
      " [-0.03047543  0.21196784  0.19984283 ... -0.0361953  -0.12393437\n",
      "  -0.09696677]\n",
      " [ 0.04469568  0.14313272  0.12914991 ...  0.16201435 -0.20495011\n",
      "  -0.057234  ]\n",
      " [-0.05428446  0.04482391 -0.15670119 ...  0.0292372  -0.07523577\n",
      "   0.04680247]]\n",
      "参数3: adult_encoder_1/dense_23/bias:0, 形状: (20,)\n",
      "[-0.00920067  0.0350136   0.0021284  -0.02661099  0.00777315  0.00156091\n",
      " -0.02564557 -0.00231723  0.0281398   0.01881186 -0.01668821 -0.0112457\n",
      "  0.01185295  0.01790676  0.02188708  0.00687731  0.01316974  0.01089815\n",
      " -0.01483435  0.04514839]\n"
     ]
    }
   ],
   "source": [
    "# 输出encoder的全部权重（包括隐藏层）\n",
    "for i, weight in enumerate(heae.encoder.weights):\n",
    "    print(f\"参数{i}: {weight.name}, 形状: {weight.shape}\")\n",
    "    print(weight.numpy())  # 打印具体数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d0db089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "COEFF_SPARSITY = 0.5               # sparisty coefficient\n",
    "COEFF_CONSISTENCY = 0.5            # consisteny coefficient\n",
    "TRAIN_STEPS = 10000               # number of training steps -> consider increasing the number of steps\n",
    "BATCH_SIZE = 100                   # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d352c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "class RewardCallback(Callback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRL,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        \n",
    "        if (step + update) % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        # get the counterfactual and target\n",
    "        Y_t = sample[\"Y_t\"]\n",
    "        X_cf = model.params[\"decoder_inv_preprocessor\"](sample[\"X_cf\"])\n",
    "        \n",
    "        # get prediction label\n",
    "        Y_m_cf = predictor(X_cf)\n",
    "        \n",
    "        # compute reward\n",
    "        reward = np.mean(model.params[\"reward_func\"](Y_m_cf, Y_t))\n",
    "        wandb.log({\"reward\": reward})\n",
    "\n",
    "class LossCallback(Callback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRL,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        # Log training losses.\n",
    "        if (step + update) % 100 == 0:\n",
    "            wandb.log(losses)\n",
    "\n",
    "class TablesCallback(Callback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRL,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        # Log every 1000 steps\n",
    "        if step % 1000 != 0:\n",
    "            return\n",
    "        \n",
    "        # Define number of samples to be displayed.\n",
    "        NUM_SAMPLES = 5\n",
    "        \n",
    "        X = heae_inv_preprocessor(sample[\"X\"][:NUM_SAMPLES])        # input instance\n",
    "        X_cf = heae_inv_preprocessor(sample[\"X_cf\"][:NUM_SAMPLES])  # counterfactual\n",
    "        \n",
    "        Y_m = np.argmax(sample[\"Y_m\"][:NUM_SAMPLES], axis=1).astype(int).reshape(-1, 1) # input labels\n",
    "        Y_t = np.argmax(sample[\"Y_t\"][:NUM_SAMPLES], axis=1).astype(int).reshape(-1, 1) # target labels\n",
    "        Y_m_cf = np.argmax(predictor(X_cf), axis=1).astype(int).reshape(-1, 1)          # counterfactual labels\n",
    "        \n",
    "        # Define feature names and category map for input.\n",
    "        feature_names = heart.feature_names + [\"Label\"]\n",
    "        category_map = deepcopy(heart.category_map)\n",
    "        category_map.update({feature_names.index(\"Label\"): heart.target_names})\n",
    "        \n",
    "        # Construct input array.\n",
    "        inputs = np.concatenate([X, Y_m], axis=1)\n",
    "        inputs = pd.DataFrame(apply_category_mapping(inputs, category_map),\n",
    "                              columns=feature_names)\n",
    "        \n",
    "        # Define feature names and category map for counterfactual output.\n",
    "        feature_names += [\"Target\"]\n",
    "        category_map.update({feature_names.index(\"Target\"): heart.target_names})\n",
    "        \n",
    "        # Construct output array.\n",
    "        outputs = np.concatenate([X_cf, Y_m_cf, Y_t], axis=1)\n",
    "        outputs = pd.DataFrame(apply_category_mapping(outputs, category_map),\n",
    "                               columns=feature_names)\n",
    "        \n",
    "        # Log table.\n",
    "        wandb.log({\n",
    "            \"Input\": wandb.Table(dataframe=inputs),\n",
    "            \"Output\": wandb.Table(dataframe=outputs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69cad1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ma1227872681\u001b[0m (\u001b[33ma1227872681-university-of-bristol\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Graduation_Project\\DiCE_XAI_codes\\wandb\\run-20250721_095911-9ofuisze</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning/runs/9ofuisze' target=\"_blank\">resilient-aardvark-25</a></strong> to <a href='https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning' target=\"_blank\">https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning/runs/9ofuisze' target=\"_blank\">https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning/runs/9ofuisze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define immutable features.\n",
    "immutable_features = ['sex']\n",
    "\n",
    "# Define ranges. This means that the `Age` feature can not decrease.\n",
    "ranges = {'age': [0.0, 1.0]}\n",
    "\n",
    "# Initialize wandb.\n",
    "wandb_project = \"Adult Census Counterfactual with Reinforcement Learning\"\n",
    "wandb.init(project=wandb_project)\n",
    "\n",
    "explainer = CounterfactualRLTabular(predictor=predictor,\n",
    "                                    encoder=heae.encoder,\n",
    "                                    decoder=heae.decoder,\n",
    "                                    latent_dim=LATENT_DIM,\n",
    "                                    encoder_preprocessor=heae_preprocessor,\n",
    "                                    decoder_inv_preprocessor=heae_inv_preprocessor,\n",
    "                                    coeff_sparsity=COEFF_SPARSITY,\n",
    "                                    coeff_consistency=COEFF_CONSISTENCY,\n",
    "                                    category_map=heart.category_map,\n",
    "                                    feature_names=heart.feature_names,\n",
    "                                    immutable_features=immutable_features,\n",
    "                                    ranges=ranges,\n",
    "                                    train_steps=TRAIN_STEPS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    backend=\"tensorflow\",\n",
    "                                    callbacks=[LossCallback(), RewardCallback(), TablesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa3042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [10:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>█▄▄▃▂▅▂▄▄▅▄▂▄▄▁▂▃▄▄▄▃▃▃▃▃▃▃▄▃▄▄▃▄▃▃▂▄▄▂▄</td></tr><tr><td>consistency_loss</td><td>█▅▄▄▄▄▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▂▁▁▂▁▁▁▁▂▁▁▂▁▁▁▁▂</td></tr><tr><td>critic_loss</td><td>██▄▅▄▂▃▃▃▂▄▂▃▂▄▁▃▂▃▁▂▂▃▁▂▁▁▂▂▃▂▂▂▂▁▂▁▁▁▂</td></tr><tr><td>reward</td><td>▁█▅▆▇▇▇█▅▇▆▅▅▆▆▇▇▆▆▇▇▇▇▇▇▆▇▅▇▅▇▇▇▇▇▇▆▇▆▆</td></tr><tr><td>sparsity_cat_loss</td><td>▆█▆▇▅▃▃▄▄▅▄▃▄▃▄▂▃▃▂▃▂▃▄▄▃▄▃▂▂▂▂▁▁▃▃▂▂▂▂▃</td></tr><tr><td>sparsity_num_loss</td><td>█▇▅▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▂▂▁▁▂▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>-0.79029</td></tr><tr><td>consistency_loss</td><td>0.08625</td></tr><tr><td>critic_loss</td><td>0.02518</td></tr><tr><td>reward</td><td>0.79</td></tr><tr><td>sparsity_cat_loss</td><td>0.2475</td></tr><tr><td>sparsity_num_loss</td><td>0.09106</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-aardvark-25</strong> at: <a href='https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning/runs/9ofuisze' target=\"_blank\">https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning/runs/9ofuisze</a><br> View project at: <a href='https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning' target=\"_blank\">https://wandb.ai/a1227872681-university-of-bristol/Adult%20Census%20Counterfactual%20with%20Reinforcement%20Learning</a><br>Synced 5 W&B file(s), 18 media file(s), 36 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250721_095911-9ofuisze\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fit the explainer.\n",
    "explainer = explainer.fit(X=X_train)\n",
    "\n",
    "# Close wandb.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ae4849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate counterfactual instances.\n",
    "sample_num = 84\n",
    "X = X_train[sample_num].reshape(1, -1)\n",
    "Y_t = np.array([1 - Y_train[sample_num]])\n",
    "C = [{\n",
    "'age': [0, 20]       # 限定 age 在 0~70\n",
    "}]\n",
    "explanation = explainer.explain(X=X, Y_t=Y_t, C=C, diversity=True, num_samples=4, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf19e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cf = explanation.data['cf']['X']\n",
    "X_cf[:, 9] = X_cf[:, 9].astype(float).round(1)\n",
    "predictor(X_cf).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c643fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat labels to the original instances.\n",
    "orig = np.concatenate(\n",
    "    [explanation.data['orig']['X'], explanation.data['orig']['class']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Concat labels to the counterfactual instances.\n",
    "cf = np.concatenate(\n",
    "    [X_cf, explanation.data['cf']['class']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Define new feature names and category map by including the label.\n",
    "feature_names = heart.feature_names + [\"target\"]\n",
    "category_map = deepcopy(heart.category_map)\n",
    "category_map.update({feature_names.index(\"target\"): heart.target_names})\n",
    "\n",
    "# Replace label encodings with strings.\n",
    "orig_pd = pd.DataFrame(\n",
    "    apply_category_mapping(orig, category_map),\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "cf_pd = pd.DataFrame(\n",
    "    apply_category_mapping(cf, category_map),\n",
    "    columns=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb86eb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex cp trestbps   chol fbs restecg thalach exang oldpeak slope ca  \\\n",
       "0  54.0   1  4    140.0  239.0   0       0   160.0     0     1.2     1  0   \n",
       "\n",
       "     thal target  \n",
       "0  normal    0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_pd.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "159e8137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>141</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>141</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age sex cp trestbps chol fbs restecg thalach exang  oldpeak slope ca  \\\n",
       "0  54   1  4      140  234   0       0     159     1      1.4     2  2   \n",
       "1  54   1  4      140  234   0       0     159     1      1.4     2  2   \n",
       "2  54   1  4      141  233   0       0     158     1      1.3     1  3   \n",
       "3  54   1  4      141  233   0       0     159     1      1.4     2  2   \n",
       "\n",
       "         thal target  \n",
       "0  reversible      1  \n",
       "1  reversible      1  \n",
       "2  reversible      1  \n",
       "3  reversible      1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_pd['oldpeak'] = pd.to_numeric(cf_pd['oldpeak'], errors='coerce').round(1)\n",
    "cf_pd.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f26ba273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.96it/s]\n",
      "1it [00:00,  9.80it/s]\n",
      "1it [00:00, 10.76it/s]\n",
      "1it [00:00, 10.23it/s]\n",
      "1it [00:00,  9.53it/s]\n",
      "1it [00:00, 11.08it/s]\n",
      "1it [00:00, 10.31it/s]\n",
      "1it [00:00, 10.97it/s]\n",
      "1it [00:00, 10.48it/s]\n",
      "1it [00:00, 11.06it/s]\n",
      "1it [00:00, 11.43it/s]\n",
      "1it [00:00, 10.64it/s]\n",
      "1it [00:00, 10.99it/s]\n",
      "1it [00:00, 10.79it/s]\n",
      "2it [00:00, 10.86it/s]\n",
      "1it [00:00, 10.49it/s]\n",
      "1it [00:00, 10.42it/s]\n",
      "1it [00:00, 11.46it/s]\n",
      "1it [00:00,  9.84it/s]\n",
      "1it [00:00, 10.64it/s]\n",
      "1it [00:00, 10.55it/s]\n",
      "1it [00:00,  9.74it/s]\n",
      "1it [00:00, 10.36it/s]\n",
      "1it [00:00,  9.78it/s]\n",
      "1it [00:00,  9.44it/s]\n",
      "1it [00:00,  9.78it/s]\n",
      "1it [00:00, 10.51it/s]\n",
      "2it [00:00, 10.19it/s]\n",
      "1it [00:00, 14.18it/s]\n",
      "1it [00:00, 14.01it/s]\n",
      "1it [00:00, 12.76it/s]\n",
      "1it [00:00, 10.08it/s]\n",
      "1it [00:00, 10.19it/s]\n",
      "1it [00:00,  8.54it/s]\n",
      "1it [00:00, 10.14it/s]\n",
      "1it [00:00,  8.97it/s]\n",
      "1it [00:00,  9.60it/s]\n",
      "1it [00:00,  9.63it/s]\n",
      "1it [00:00,  9.61it/s]\n",
      "1it [00:00,  9.66it/s]\n",
      "1it [00:00,  9.86it/s]\n",
      "1it [00:00, 10.38it/s]\n",
      "1it [00:00,  9.81it/s]\n",
      "1it [00:00,  9.25it/s]\n",
      "1it [00:00,  9.43it/s]\n",
      "1it [00:00,  9.00it/s]\n",
      "1it [00:00, 10.18it/s]\n",
      "2it [00:00,  9.50it/s]\n",
      "1it [00:00,  9.92it/s]\n",
      "1it [00:00, 10.27it/s]\n",
      "1it [00:00,  9.28it/s]\n",
      "1it [00:00,  9.68it/s]\n",
      "2it [00:00,  9.46it/s]\n",
      "1it [00:00, 13.11it/s]\n",
      "1it [00:00, 15.27it/s]\n",
      "1it [00:00, 10.60it/s]\n",
      "1it [00:00,  9.25it/s]\n",
      "1it [00:00,  9.25it/s]\n",
      "1it [00:00,  9.82it/s]\n",
      "1it [00:00,  9.76it/s]\n",
      "1it [00:00, 10.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*has feature names.*')\n",
    "import time\n",
    "\n",
    "cfs = []\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i, X_test_i in enumerate(X_test):\n",
    "    X = X_test_i.reshape(1, -1)\n",
    "    Y_t = 1 - predictor(X).argmax(axis=1)\n",
    "    C = [{\n",
    "    'age': [0, 20]       # 限定 age 在 0~70\n",
    "    }]\n",
    "    explanation = explainer.explain(X=X, Y_t=Y_t, C=C, diversity=True, num_samples=4, batch_size=10)\n",
    "    \n",
    "    # Concat labels to the original instances.\n",
    "    orig = np.concatenate(\n",
    "        [explanation.data['orig']['X'], explanation.data['orig']['class']],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Concat labels to the counterfactual instances.\n",
    "    cf = np.concatenate(\n",
    "        [explanation.data['cf']['X'], explanation.data['cf']['class']],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Define new feature names and category map by including the label.\n",
    "    feature_names = heart.feature_names + [\"target\"]\n",
    "    category_map = deepcopy(heart.category_map)\n",
    "    category_map.update({feature_names.index(\"target\"): heart.target_names})\n",
    "\n",
    "    # Replace label encodings with strings.\n",
    "    orig_pd = pd.DataFrame(\n",
    "        apply_category_mapping(orig, category_map),\n",
    "        columns=feature_names\n",
    "    )\n",
    "\n",
    "    cf_pd = pd.DataFrame(\n",
    "        apply_category_mapping(cf, category_map),\n",
    "        columns=feature_names\n",
    "    )\n",
    "    cf_pd['oldpeak'] = cf_pd['oldpeak'].astype(float).round(1)\n",
    "    cfs.append((\n",
    "        orig_pd, cf_pd\n",
    "    ))\n",
    "end = time.time()\n",
    "\n",
    "time_Alibi_cfrl = (end - start)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fa831ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 映射\n",
    "def custom_label_encoder(feature_names, df):\n",
    "    arr = []\n",
    "    for _, row in df.iterrows():\n",
    "        encoded_row = []\n",
    "        for i, f in enumerate(feature_names):\n",
    "            if i in category_map:\n",
    "                val = row[f]\n",
    "                idx = category_map[i].index(val)\n",
    "                encoded_row.append(idx)\n",
    "            else:\n",
    "                encoded_row.append(float(row[f]))\n",
    "        arr.append(encoded_row)\n",
    "    # 返回二维 array，类型 float32\n",
    "    return np.array(arr)[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b783df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_count = 0\n",
    "total = 0\n",
    "for X_org, X_cfs in cfs:\n",
    "    # 预测原始类别（X_org必须二维）\n",
    "    y_true = model.predict(preprocessor.transform(custom_label_encoder(feature_names, X_org)), \n",
    "                           verbose=0).argmax(axis=1)[0]\n",
    "    # 预测反事实类别（批量）\n",
    "    y_cfs = model.predict(preprocessor.transform(custom_label_encoder(feature_names, X_cfs)), \n",
    "                          verbose=0).argmax(axis=1)\n",
    "    # 统计类别发生变化的样本数\n",
    "    valid_count += np.sum(y_cfs != y_true)\n",
    "    total += len(y_cfs)\n",
    "valid_alibi_cfrl = (valid_count / total if total > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab57175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from XAI_metrics import calc_sparsity, calc_continuous_proximity, \\\n",
    "    calc_categorical_proximity, calc_manifold_distance, calc_cf_num\n",
    "\n",
    "\n",
    "sparsity_alibi_cfrl = calc_sparsity(cfs, categorical_features)\n",
    "con_proximity_alibi_cfrl = calc_continuous_proximity(cfs, numerical_names)\n",
    "cat_proximity_alibi_cfrl = calc_categorical_proximity(cfs, categorical_features)\n",
    "manifold_alibi_cfrl = calc_manifold_distance(cfs, df, categorical_features)\n",
    "cf_num_alibi_cfrl = calc_cf_num(cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305176fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>Avg Time(s)</th>\n",
       "      <th>Validity</th>\n",
       "      <th>Sparsity</th>\n",
       "      <th>Proximity_con</th>\n",
       "      <th>Proximity_cat</th>\n",
       "      <th>Manifold</th>\n",
       "      <th>Avg CF count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alibi_CFRL</td>\n",
       "      <td>0.234381</td>\n",
       "      <td>0.995902</td>\n",
       "      <td>0.567062</td>\n",
       "      <td>0.362365</td>\n",
       "      <td>0.359631</td>\n",
       "      <td>9.748427</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       method  Avg Time(s)  Validity  Sparsity  Proximity_con  Proximity_cat  \\\n",
       "0  Alibi_CFRL     0.234381  0.995902  0.567062       0.362365       0.359631   \n",
       "\n",
       "   Manifold  Avg CF count  \n",
       "0  9.748427           4.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_alibi_cfrl = {\n",
    "    \"method\": [\"Alibi_CFRL\"],\n",
    "    \"Avg Time(s)\": [time_Alibi_cfrl],\n",
    "    \"Validity\": [valid_alibi_cfrl],\n",
    "    \"Sparsity\": [sparsity_alibi_cfrl],\n",
    "    \"Proximity_con\": [con_proximity_alibi_cfrl],\n",
    "    \"Proximity_cat\": [cat_proximity_alibi_cfrl],\n",
    "    \"Manifold\": [manifold_alibi_cfrl],\n",
    "    \"Avg CF count\": [cf_num_alibi_cfrl]\n",
    "}\n",
    "\n",
    "df_results_alibi_cfrl = pd.DataFrame(results_alibi_cfrl)\n",
    "df_results_alibi_cfrl = df_results_alibi_cfrl.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf2bf17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_alibi_cfrl.to_csv('./results/Alibi_CFRL_result_heart.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dice_XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
