{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a0ae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\dice_XAI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from alibi.explainers import CounterfactualRLTabular, CounterfactualRL\n",
    "from alibi.datasets import fetch_adult\n",
    "from alibi.models.tensorflow import HeAE\n",
    "from alibi.models.tensorflow import Actor, Critic\n",
    "from alibi.models.tensorflow import ADULTEncoder, ADULTDecoder\n",
    "from alibi.explainers.cfrl_base import Callback\n",
    "from alibi.explainers.backends.cfrl_tabular import get_he_preprocessor, get_statistics, \\\n",
    "    get_conditional_vector, apply_category_mapping\n",
    "import random\n",
    "\n",
    "def set_seed(s=0):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    tf.random.set_seed(s)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d79456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional, Tuple, Union\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from alibi.utils.data import Bunch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "df = pd.read_csv('E:\\Graduation_Project\\datasets\\heart.csv')\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "df[categorical_features] = df[categorical_features].astype(str)\n",
    "\n",
    "\n",
    "def fetch_heart(\n",
    "    features_drop: Optional[list] = None, \n",
    "    return_X_y: bool = False, \n",
    "    categorical_features: Optional[list] = None\n",
    ") -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]:\n",
    "    \n",
    "    raw_data = df\n",
    "    # get labels, features and drop unnecessary features\n",
    "    labels = (raw_data['target'] == 1).astype(int).values\n",
    "\n",
    "    if features_drop is None:\n",
    "        features_drop = []\n",
    "    features_drop = features_drop + ['target']\n",
    "\n",
    "    data = raw_data.drop(features_drop, axis=1)\n",
    "    features = list(data.columns)\n",
    "\n",
    "   \n",
    "    if categorical_features is None:\n",
    "        categorical_features = [f for f in features if data[f].dtype == 'O']\n",
    "\n",
    "    category_map = {}\n",
    "    for f in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        data_tmp = le.fit_transform(data[f].values)\n",
    "        data[f] = data_tmp\n",
    "        category_map[features.index(f)] = list(le.classes_)\n",
    "\n",
    "    # only return data values\n",
    "    data = data.values\n",
    "    target_names = [0, 1]\n",
    "\n",
    "    if return_X_y:\n",
    "        return data, labels\n",
    "\n",
    "    return Bunch(\n",
    "        data=data, \n",
    "        target=labels, \n",
    "        feature_names=features, \n",
    "        target_names=target_names, \n",
    "        category_map=category_map\n",
    "    )\n",
    "\n",
    "heart = fetch_heart(\n",
    "    categorical_features=categorical_features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Separate columns in numerical and categorical.\n",
    "categorical_names = [heart.feature_names[i] for i in heart.category_map.keys()]\n",
    "categorical_ids = list(heart.category_map.keys())\n",
    "\n",
    "numerical_names = [name for i, name in enumerate(heart.feature_names) if i not in heart.category_map.keys()]\n",
    "numerical_ids = [i for i in range(len(heart.feature_names)) if i not in heart.category_map.keys()]\n",
    "\n",
    "# Split data into train and test\n",
    "X, Y = heart.data, heart.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f061d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "# Define numerical standard scaler.\n",
    "num_transf = MinMaxScaler()\n",
    "\n",
    "# Define categorical one-hot encoder.\n",
    "cat_transf = OneHotEncoder(\n",
    "    categories=[range(len(x)) for x in heart.category_map.values()],\n",
    "    handle_unknown=\"ignore\"\n",
    ")\n",
    "\n",
    "# Define column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transf, numerical_ids),\n",
    "        (\"cat\", cat_transf, categorical_ids),\n",
    "    ],\n",
    "    sparse_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2d9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit preprocessor.\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Preprocess train and test dataset.\n",
    "X_train_ohe = preprocessor.transform(X_train)\n",
    "X_test_ohe = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8056ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "def build_simple_dnn():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(16, activation='relu', input_shape=(31,)))  # 输入31维特征\n",
    "    model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "model = build_simple_dnn()\n",
    "model.load_weights('my_model_weights.h5')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f40e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84857166, 0.15142827],\n",
       "       [0.88690937, 0.11309064]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define prediction function.\n",
    "predictor = lambda x: model.predict(preprocessor.transform(x), verbose=0)\n",
    "\n",
    "predictor(X_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c04846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Compute accuracy.\n",
    "acc = accuracy_score(y_true=Y_test, y_pred=predictor(X_test).argmax(axis=1))\n",
    "print(\"Accuracy: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9da2457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'trestbps', 'chol', 'thalach', 'oldpeak']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4312416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attribute types, required for datatype conversion.\n",
    "feature_types = {'age': int, 'trestbps': int, 'chol': int, 'thalach': int, 'oldpeak': float}\n",
    "\n",
    "# Define data preprocessor and inverse preprocessor. The invers preprocessor include datatype conversions.\n",
    "heae_preprocessor, heae_inv_preprocessor = get_he_preprocessor(X=X_train,\n",
    "                                                               feature_names=heart.feature_names,\n",
    "                                                               category_map=heart.category_map,\n",
    "                                                               feature_types=feature_types)\n",
    "\n",
    "# Define trainset\n",
    "trainset_input = heae_preprocessor(X_train).astype(np.float32)\n",
    "trainset_outputs = {\n",
    "    \"output_1\": trainset_input[:, :len(numerical_ids)]\n",
    "}\n",
    "\n",
    "for i, cat_id in enumerate(categorical_ids):\n",
    "    trainset_outputs.update({\n",
    "        f\"output_{i+2}\": X_train[:, cat_id]\n",
    "    })\n",
    "    \n",
    "trainset = tf.data.Dataset.from_tensor_slices((trainset_input, trainset_outputs))\n",
    "trainset = trainset.shuffle(1024).batch(32, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8865daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define autoencoder path and create dir if it doesn't exist.\n",
    "heae_path = os.path.join(\"tensorflow_heart\", \"ADULT_autoencoder\")\n",
    "if not os.path.exists(heae_path):\n",
    "    os.makedirs(heae_path)\n",
    "\n",
    "# Define constants.\n",
    "EPOCHS = 50             # epochs to train the autoencoder\n",
    "HIDDEN_DIM = 128         # hidden dimension of the autoencoder\n",
    "LATENT_DIM = 20          # define latent dimension\n",
    "\n",
    "# Define output dimensions.\n",
    "OUTPUT_DIMS = [len(numerical_ids)]\n",
    "OUTPUT_DIMS += [len(heart.category_map[cat_id]) for cat_id in categorical_ids]\n",
    "\n",
    "# Define the heterogeneous auto-encoder.\n",
    "heae = HeAE(encoder=ADULTEncoder(hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM),\n",
    "            decoder=ADULTDecoder(hidden_dim=HIDDEN_DIM, output_dims=OUTPUT_DIMS))\n",
    "\n",
    "# Define loss functions.\n",
    "he_loss = [keras.losses.MeanSquaredError()]\n",
    "he_loss_weights = [1.]\n",
    "\n",
    "# Add categorical losses.\n",
    "for i in range(len(categorical_names)):\n",
    "    he_loss.append(keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "    he_loss_weights.append(1./len(categorical_names))\n",
    "\n",
    "# Define metrics.\n",
    "metrics = {}\n",
    "for i, cat_name in enumerate(categorical_names):\n",
    "    metrics.update({f\"output_{i+2}\": keras.metrics.SparseCategoricalAccuracy()})\n",
    "    \n",
    "# Compile model.\n",
    "heae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "             loss=he_loss,\n",
    "             loss_weights=he_loss_weights,\n",
    "             metrics=metrics)\n",
    "\n",
    "if len(os.listdir(heae_path)) == 0:\n",
    "    # Fit and save autoencoder.\n",
    "    heae.fit(trainset, epochs=EPOCHS)\n",
    "    heae.save(heae_path, save_format=\"tf\")\n",
    "else:\n",
    "    # Load the model.\n",
    "    heae = keras.models.load_model(heae_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8481fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, weight in enumerate(heae.encoder.weights):\n",
    "    print(f\"参数{i}: {weight.name}, 形状: {weight.shape}\")\n",
    "    print(weight.numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d0db089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "COEFF_SPARSITY = 0.5               # sparisty coefficient\n",
    "COEFF_CONSISTENCY = 0.5            # consisteny coefficient\n",
    "TRAIN_STEPS = 10000               # number of training steps -> consider increasing the number of steps\n",
    "BATCH_SIZE = 100                   # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d352c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "class RewardCallback(Callback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRL,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        \n",
    "        if (step + update) % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        # get the counterfactual and target\n",
    "        Y_t = sample[\"Y_t\"]\n",
    "        X_cf = model.params[\"decoder_inv_preprocessor\"](sample[\"X_cf\"])\n",
    "        \n",
    "        # get prediction label\n",
    "        Y_m_cf = predictor(X_cf)\n",
    "        \n",
    "        # compute reward\n",
    "        reward = np.mean(model.params[\"reward_func\"](Y_m_cf, Y_t))\n",
    "        wandb.log({\"reward\": reward})\n",
    "\n",
    "class LossCallback(Callback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRL,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        # Log training losses.\n",
    "        if (step + update) % 100 == 0:\n",
    "            wandb.log(losses)\n",
    "\n",
    "class TablesCallback(Callback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRL,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        # Log every 1000 steps\n",
    "        if step % 1000 != 0:\n",
    "            return\n",
    "        \n",
    "        # Define number of samples to be displayed.\n",
    "        NUM_SAMPLES = 5\n",
    "        \n",
    "        X = heae_inv_preprocessor(sample[\"X\"][:NUM_SAMPLES])        # input instance\n",
    "        X_cf = heae_inv_preprocessor(sample[\"X_cf\"][:NUM_SAMPLES])  # counterfactual\n",
    "        \n",
    "        Y_m = np.argmax(sample[\"Y_m\"][:NUM_SAMPLES], axis=1).astype(int).reshape(-1, 1) # input labels\n",
    "        Y_t = np.argmax(sample[\"Y_t\"][:NUM_SAMPLES], axis=1).astype(int).reshape(-1, 1) # target labels\n",
    "        Y_m_cf = np.argmax(predictor(X_cf), axis=1).astype(int).reshape(-1, 1)          # counterfactual labels\n",
    "        \n",
    "        # Define feature names and category map for input.\n",
    "        feature_names = heart.feature_names + [\"Label\"]\n",
    "        category_map = deepcopy(heart.category_map)\n",
    "        category_map.update({feature_names.index(\"Label\"): heart.target_names})\n",
    "        \n",
    "        # Construct input array.\n",
    "        inputs = np.concatenate([X, Y_m], axis=1)\n",
    "        inputs = pd.DataFrame(apply_category_mapping(inputs, category_map),\n",
    "                              columns=feature_names)\n",
    "        \n",
    "        # Define feature names and category map for counterfactual output.\n",
    "        feature_names += [\"Target\"]\n",
    "        category_map.update({feature_names.index(\"Target\"): heart.target_names})\n",
    "        \n",
    "        # Construct output array.\n",
    "        outputs = np.concatenate([X_cf, Y_m_cf, Y_t], axis=1)\n",
    "        outputs = pd.DataFrame(apply_category_mapping(outputs, category_map),\n",
    "                               columns=feature_names)\n",
    "        \n",
    "        # Log table.\n",
    "        wandb.log({\n",
    "            \"Input\": wandb.Table(dataframe=inputs),\n",
    "            \"Output\": wandb.Table(dataframe=outputs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cad1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define immutable features.\n",
    "immutable_features = ['sex']\n",
    "\n",
    "# Define ranges. This means that the `Age` feature can not decrease.\n",
    "ranges = {'age': [0.0, 1.0]}\n",
    "\n",
    "# Initialize wandb.\n",
    "wandb_project = \"Adult Census Counterfactual with Reinforcement Learning\"\n",
    "wandb.init(project=wandb_project)\n",
    "\n",
    "explainer = CounterfactualRLTabular(predictor=predictor,\n",
    "                                    encoder=heae.encoder,\n",
    "                                    decoder=heae.decoder,\n",
    "                                    latent_dim=LATENT_DIM,\n",
    "                                    encoder_preprocessor=heae_preprocessor,\n",
    "                                    decoder_inv_preprocessor=heae_inv_preprocessor,\n",
    "                                    coeff_sparsity=COEFF_SPARSITY,\n",
    "                                    coeff_consistency=COEFF_CONSISTENCY,\n",
    "                                    category_map=heart.category_map,\n",
    "                                    feature_names=heart.feature_names,\n",
    "                                    immutable_features=immutable_features,\n",
    "                                    ranges=ranges,\n",
    "                                    train_steps=TRAIN_STEPS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    backend=\"tensorflow\",\n",
    "                                    callbacks=[LossCallback(), RewardCallback(), TablesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fit the explainer.\n",
    "explainer = explainer.fit(X=X_train)\n",
    "\n",
    "# Close wandb.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate counterfactual instances.\n",
    "sample_num = 84\n",
    "X = X_train[sample_num].reshape(1, -1)\n",
    "Y_t = np.array([1 - Y_train[sample_num]])\n",
    "C = [{\n",
    "'age': [0, 20]     \n",
    "}]\n",
    "explanation = explainer.explain(X=X, Y_t=Y_t, C=C, diversity=True, num_samples=4, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf19e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cf = explanation.data['cf']['X']\n",
    "X_cf[:, 9] = X_cf[:, 9].astype(float).round(1)\n",
    "predictor(X_cf).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c643fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat labels to the original instances.\n",
    "orig = np.concatenate(\n",
    "    [explanation.data['orig']['X'], explanation.data['orig']['class']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Concat labels to the counterfactual instances.\n",
    "cf = np.concatenate(\n",
    "    [X_cf, explanation.data['cf']['class']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Define new feature names and category map by including the label.\n",
    "feature_names = heart.feature_names + [\"target\"]\n",
    "category_map = deepcopy(heart.category_map)\n",
    "category_map.update({feature_names.index(\"target\"): heart.target_names})\n",
    "\n",
    "# Replace label encodings with strings.\n",
    "orig_pd = pd.DataFrame(\n",
    "    apply_category_mapping(orig, category_map),\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "cf_pd = pd.DataFrame(\n",
    "    apply_category_mapping(cf, category_map),\n",
    "    columns=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb86eb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex cp trestbps   chol fbs restecg thalach exang oldpeak slope ca  \\\n",
       "0  54.0   1  4    140.0  239.0   0       0   160.0     0     1.2     1  0   \n",
       "\n",
       "     thal target  \n",
       "0  normal    0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_pd.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "159e8137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>141</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>141</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reversible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age sex cp trestbps chol fbs restecg thalach exang  oldpeak slope ca  \\\n",
       "0  54   1  4      140  234   0       0     159     1      1.4     2  2   \n",
       "1  54   1  4      140  234   0       0     159     1      1.4     2  2   \n",
       "2  54   1  4      141  233   0       0     158     1      1.3     1  3   \n",
       "3  54   1  4      141  233   0       0     159     1      1.4     2  2   \n",
       "\n",
       "         thal target  \n",
       "0  reversible      1  \n",
       "1  reversible      1  \n",
       "2  reversible      1  \n",
       "3  reversible      1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_pd['oldpeak'] = pd.to_numeric(cf_pd['oldpeak'], errors='coerce').round(1)\n",
    "cf_pd.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ba273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.96it/s]\n",
      "1it [00:00,  9.80it/s]\n",
      "1it [00:00, 10.76it/s]\n",
      "1it [00:00, 10.23it/s]\n",
      "1it [00:00,  9.53it/s]\n",
      "1it [00:00, 11.08it/s]\n",
      "1it [00:00, 10.31it/s]\n",
      "1it [00:00, 10.97it/s]\n",
      "1it [00:00, 10.48it/s]\n",
      "1it [00:00, 11.06it/s]\n",
      "1it [00:00, 11.43it/s]\n",
      "1it [00:00, 10.64it/s]\n",
      "1it [00:00, 10.99it/s]\n",
      "1it [00:00, 10.79it/s]\n",
      "2it [00:00, 10.86it/s]\n",
      "1it [00:00, 10.49it/s]\n",
      "1it [00:00, 10.42it/s]\n",
      "1it [00:00, 11.46it/s]\n",
      "1it [00:00,  9.84it/s]\n",
      "1it [00:00, 10.64it/s]\n",
      "1it [00:00, 10.55it/s]\n",
      "1it [00:00,  9.74it/s]\n",
      "1it [00:00, 10.36it/s]\n",
      "1it [00:00,  9.78it/s]\n",
      "1it [00:00,  9.44it/s]\n",
      "1it [00:00,  9.78it/s]\n",
      "1it [00:00, 10.51it/s]\n",
      "2it [00:00, 10.19it/s]\n",
      "1it [00:00, 14.18it/s]\n",
      "1it [00:00, 14.01it/s]\n",
      "1it [00:00, 12.76it/s]\n",
      "1it [00:00, 10.08it/s]\n",
      "1it [00:00, 10.19it/s]\n",
      "1it [00:00,  8.54it/s]\n",
      "1it [00:00, 10.14it/s]\n",
      "1it [00:00,  8.97it/s]\n",
      "1it [00:00,  9.60it/s]\n",
      "1it [00:00,  9.63it/s]\n",
      "1it [00:00,  9.61it/s]\n",
      "1it [00:00,  9.66it/s]\n",
      "1it [00:00,  9.86it/s]\n",
      "1it [00:00, 10.38it/s]\n",
      "1it [00:00,  9.81it/s]\n",
      "1it [00:00,  9.25it/s]\n",
      "1it [00:00,  9.43it/s]\n",
      "1it [00:00,  9.00it/s]\n",
      "1it [00:00, 10.18it/s]\n",
      "2it [00:00,  9.50it/s]\n",
      "1it [00:00,  9.92it/s]\n",
      "1it [00:00, 10.27it/s]\n",
      "1it [00:00,  9.28it/s]\n",
      "1it [00:00,  9.68it/s]\n",
      "2it [00:00,  9.46it/s]\n",
      "1it [00:00, 13.11it/s]\n",
      "1it [00:00, 15.27it/s]\n",
      "1it [00:00, 10.60it/s]\n",
      "1it [00:00,  9.25it/s]\n",
      "1it [00:00,  9.25it/s]\n",
      "1it [00:00,  9.82it/s]\n",
      "1it [00:00,  9.76it/s]\n",
      "1it [00:00, 10.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*has feature names.*')\n",
    "import time\n",
    "\n",
    "cfs = []\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i, X_test_i in enumerate(X_test):\n",
    "    X = X_test_i.reshape(1, -1)\n",
    "    Y_t = 1 - predictor(X).argmax(axis=1)\n",
    "    C = [{\n",
    "    'age': [0, 20]      \n",
    "    }]\n",
    "    explanation = explainer.explain(X=X, Y_t=Y_t, C=C, diversity=True, num_samples=4, batch_size=10)\n",
    "    \n",
    "    # Concat labels to the original instances.\n",
    "    orig = np.concatenate(\n",
    "        [explanation.data['orig']['X'], explanation.data['orig']['class']],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Concat labels to the counterfactual instances.\n",
    "    cf = np.concatenate(\n",
    "        [explanation.data['cf']['X'], explanation.data['cf']['class']],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Define new feature names and category map by including the label.\n",
    "    feature_names = heart.feature_names + [\"target\"]\n",
    "    category_map = deepcopy(heart.category_map)\n",
    "    category_map.update({feature_names.index(\"target\"): heart.target_names})\n",
    "\n",
    "    # Replace label encodings with strings.\n",
    "    orig_pd = pd.DataFrame(\n",
    "        apply_category_mapping(orig, category_map),\n",
    "        columns=feature_names\n",
    "    )\n",
    "\n",
    "    cf_pd = pd.DataFrame(\n",
    "        apply_category_mapping(cf, category_map),\n",
    "        columns=feature_names\n",
    "    )\n",
    "    cf_pd['oldpeak'] = cf_pd['oldpeak'].astype(float).round(1)\n",
    "    cfs.append((\n",
    "        orig_pd, cf_pd\n",
    "    ))\n",
    "end = time.time()\n",
    "\n",
    "time_Alibi_cfrl = (end - start)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa831ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_label_encoder(feature_names, df):\n",
    "    arr = []\n",
    "    for _, row in df.iterrows():\n",
    "        encoded_row = []\n",
    "        for i, f in enumerate(feature_names):\n",
    "            if i in category_map:\n",
    "                val = row[f]\n",
    "                idx = category_map[i].index(val)\n",
    "                encoded_row.append(idx)\n",
    "            else:\n",
    "                encoded_row.append(float(row[f]))\n",
    "        arr.append(encoded_row)\n",
    "    \n",
    "    return np.array(arr)[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_count = 0\n",
    "total = 0\n",
    "for X_org, X_cfs in cfs:\n",
    "    \n",
    "    y_true = model.predict(preprocessor.transform(custom_label_encoder(feature_names, X_org)), \n",
    "                           verbose=0).argmax(axis=1)[0]\n",
    "    \n",
    "    y_cfs = model.predict(preprocessor.transform(custom_label_encoder(feature_names, X_cfs)), \n",
    "                          verbose=0).argmax(axis=1)\n",
    "  \n",
    "    valid_count += np.sum(y_cfs != y_true)\n",
    "    total += len(y_cfs)\n",
    "valid_alibi_cfrl = (valid_count / total if total > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab57175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from XAI_metrics import calc_sparsity, calc_continuous_proximity, \\\n",
    "    calc_categorical_proximity, calc_manifold_distance, calc_cf_num\n",
    "\n",
    "\n",
    "sparsity_alibi_cfrl = calc_sparsity(cfs, categorical_features)\n",
    "con_proximity_alibi_cfrl = calc_continuous_proximity(cfs, numerical_names)\n",
    "cat_proximity_alibi_cfrl = calc_categorical_proximity(cfs, categorical_features)\n",
    "manifold_alibi_cfrl = calc_manifold_distance(cfs, df, categorical_features)\n",
    "cf_num_alibi_cfrl = calc_cf_num(cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305176fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>Avg Time(s)</th>\n",
       "      <th>Validity</th>\n",
       "      <th>Sparsity</th>\n",
       "      <th>Proximity_con</th>\n",
       "      <th>Proximity_cat</th>\n",
       "      <th>Manifold</th>\n",
       "      <th>Avg CF count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alibi_CFRL</td>\n",
       "      <td>0.234381</td>\n",
       "      <td>0.995902</td>\n",
       "      <td>0.567062</td>\n",
       "      <td>0.362365</td>\n",
       "      <td>0.359631</td>\n",
       "      <td>9.748427</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       method  Avg Time(s)  Validity  Sparsity  Proximity_con  Proximity_cat  \\\n",
       "0  Alibi_CFRL     0.234381  0.995902  0.567062       0.362365       0.359631   \n",
       "\n",
       "   Manifold  Avg CF count  \n",
       "0  9.748427           4.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_alibi_cfrl = {\n",
    "    \"method\": [\"Alibi_CFRL\"],\n",
    "    \"Avg Time(s)\": [time_Alibi_cfrl],\n",
    "    \"Validity\": [valid_alibi_cfrl],\n",
    "    \"Sparsity\": [sparsity_alibi_cfrl],\n",
    "    \"Proximity_con\": [con_proximity_alibi_cfrl],\n",
    "    \"Proximity_cat\": [cat_proximity_alibi_cfrl],\n",
    "    \"Manifold\": [manifold_alibi_cfrl],\n",
    "    \"Avg CF count\": [cf_num_alibi_cfrl]\n",
    "}\n",
    "\n",
    "df_results_alibi_cfrl = pd.DataFrame(results_alibi_cfrl)\n",
    "df_results_alibi_cfrl = df_results_alibi_cfrl.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf2bf17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_alibi_cfrl.to_csv('./results/Alibi_CFRL_result_heart.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dice_XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
